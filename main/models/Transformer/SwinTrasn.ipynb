{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b370745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/300, Train Loss: 0.3152, Val Loss: 0.1805, LR: 0.000100\n",
      "Validation loss decreased to 0.1805. Saving model...\n",
      "Epoch 2/300, Train Loss: 0.1694, Val Loss: 0.1781, LR: 0.000100\n",
      "Validation loss decreased to 0.1781. Saving model...\n",
      "Epoch 3/300, Train Loss: 0.1607, Val Loss: 0.1831, LR: 0.000100\n",
      "Epoch 4/300, Train Loss: 0.1305, Val Loss: 0.1652, LR: 0.000100\n",
      "Validation loss decreased to 0.1652. Saving model...\n",
      "Epoch 5/300, Train Loss: 0.1110, Val Loss: 0.1839, LR: 0.000100\n",
      "Epoch 6/300, Train Loss: 0.0990, Val Loss: 0.2065, LR: 0.000100\n",
      "Epoch 7/300, Train Loss: 0.0827, Val Loss: 0.2048, LR: 0.000100\n",
      "Epoch 8/300, Train Loss: 0.0852, Val Loss: 0.2141, LR: 0.000010\n",
      "Epoch 9/300, Train Loss: 0.0499, Val Loss: 0.2231, LR: 0.000010\n",
      "Early stopping triggered! Validation loss has not improved for 5 epochs.\n",
      "Early stopping at epoch 9\n",
      "Training log saved to training_log.csv\n",
      "\n",
      "Loading best model for testing...\n",
      "Starting evaluation on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9872\\1843193495.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Test Results =====\n",
      "Accuracy on test set: 92.13%\n",
      "F1 Score on test set: 0.9188\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Import scheduler\n",
    "import pandas as pd # Import pandas for logging\n",
    "\n",
    "# ==== Cài đặt chung ====\n",
    "data_dir = r\"C:\\Users\\Admin\\Documents\\Python Project\\Res conn 2025\\final_data\\not_seg\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_save_path = 'best_swin_model.pth' # Đường dẫn để lưu mô hình tốt nhất\n",
    "log_file_path = 'training_log.csv' # Đường dẫn để lưu file log\n",
    "\n",
    "\n",
    "# ==== Tiền xử lý Dữ liệu ====\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load dataset với transform mặc định để chia dataset\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=val_test_transform)\n",
    "\n",
    "# ==== Chia dataset thành train, val, test ====\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Gán transform riêng biệt cho từng tập con\n",
    "# Lưu ý: Các Subset (train_dataset, val_dataset, test_dataset)\n",
    "# tham chiếu đến cùng một đối tượng dataset gốc.\n",
    "# Việc thay đổi transform của 'dataset' trong mỗi Subset sẽ áp dụng\n",
    "# cho các mẫu được lấy ra từ Subset đó.\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = val_test_transform\n",
    "test_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "\n",
    "# ==== DataLoader ====\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count()//2 if os.cpu_count() else 0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=os.cpu_count()//2 if os.cpu_count() else 0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=os.cpu_count()//2 if os.cpu_count() else 0)\n",
    "\n",
    "# ==== Cấu hình Mô hình ViT ====\n",
    "# Tạo mô hình Swin Transformer với pretrained weights\n",
    "# num_classes được đặt bằng số lượng lớp trong dataset của bạn\n",
    "model = create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=len(full_dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Định nghĩa hàm mất mát và bộ tối ưu hóa\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
    "\n",
    "# ==== Learning Rate Scheduler ====\n",
    "# Giảm Learning Rate khi validation loss không cải thiện sau 'patience' epoch\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# ==== EarlyStopping ====\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=4, min_delta=0, path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = np.inf\n",
    "        self.counter = 0\n",
    "        self.path = path # Đường dẫn để lưu mô hình tốt nhất\n",
    "\n",
    "    def __call__(self, val_loss, model_state_dict): # Thêm đối số model_state_dict\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model_state_dict, self.path) # Lưu trạng thái mô hình\n",
    "            print(f\"Validation loss decreased to {val_loss:.4f}. Saving model...\")\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Early stopping triggered! Validation loss has not improved for {self.patience} epochs.\")\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001, path=model_save_path)\n",
    "\n",
    "# ==== Training Loop ====\n",
    "epochs = 300\n",
    "log_data = [] # Danh sách để lưu dữ liệu log\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # --- Giai đoạn Huấn luyện ---\n",
    "    model.train() # Đặt mô hình về chế độ huấn luyện\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # Đặt gradient về 0\n",
    "        outputs = model(imgs) # Lan truyền tiến\n",
    "        loss = criterion(outputs, labels) # Tính toán mất mát\n",
    "        loss.backward() # Lan truyền ngược\n",
    "        optimizer.step() # Cập nhật trọng số\n",
    "        \n",
    "        running_loss += loss.item() * imgs.size(0) # Cộng dồn loss (nhân với batch size để có tổng loss)\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset) # Loss trung bình trên tập huấn luyện\n",
    "\n",
    "    # --- Giai đoạn Đánh giá trên tập Validation ---\n",
    "    model.eval() # Đặt mô hình về chế độ đánh giá\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): # Tắt tính toán gradient trong giai đoạn này\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset) # Loss trung bình trên tập validation\n",
    "\n",
    "    # Cập nhật Learning Rate Scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Ghi log dữ liệu\n",
    "    log_data.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'learning_rate': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "    # Kiểm tra Early Stopping\n",
    "    if early_stopping(val_loss, model.state_dict()): # Truyền trạng thái mô hình\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Lưu log quá trình huấn luyện vào file CSV\n",
    "df_log = pd.DataFrame(log_data)\n",
    "df_log.to_csv(log_file_path, index=False)\n",
    "print(f\"Training log saved to {log_file_path}\")\n",
    "\n",
    "\n",
    "print(\"\\nLoading best model for testing...\")\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"Starting evaluation on test set...\")\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "print(f\"\\n===== Test Results =====\")\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")\n",
    "print(f\"F1 Score on test set: {f1:.4f}\")\n",
    "print(\"========================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
